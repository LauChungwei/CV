· 总结
  卷积神经网络CNN主要用来识别位移、缩放及其他形式扭曲不变性的二维图形。由于CNN的特征检测层通过训练数据进行学习，所以在使用CNN时，避免了显式的特征抽取，
而隐式地从训练数据中进行学习；再者由于同一特征映射面上的神经元权值相同，所以网络可以并行学习，这也是卷积网络相对于神经元彼此相连网络的一大优势。
卷积神经网络以其局部权值共享的特殊结构在语音识别和图像处理方面有着独特的优越性，其布局更接近于实际的生物神经网络，权值共享降低了网络的复杂性，
特别是多维输入向量的图像可以直接输入网络这一特点避免了特征提取和分类过程中数据重建的复杂度。



· 卷积神经网络的层级结构
    •	数据输入层/ Input layer
　　•	卷积计算层/ CONV layer
　　•	ReLU激励层 / ReLU layer
　　•	池化层 / Pooling layer
　　•	全连接层 / FC layer
  

第一个问题：为什么不用BP神经网络去做呢？
1.全连接，权值太多，需要很多样本去训练，计算困难
　　•	应对之道：减少权值的尝试，局部连接，权值共享

　　卷积神经网络有两种神器可以降低参数数目。
　　第一种神器叫做局部感知野，一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。
　　第二级神器，即权值共享。

 


2.边缘过渡不平滑
　　•	应对之道：采样窗口彼此重叠
 


第二个问题：LeNet里的隐层的神经元个数怎么确定呢？
它和原图像，也就是输入的大小（神经元个数）、滤波器的大小和滤波器在图像中的滑动步长都有关！

LeNet-5共有7层，不包含输入，每层都包含可训练参数（连接权重）。输入图像为32*32大小。


例如，我的图像是1000x1000像素，而滤波器大小是10x10，假设滤波器没有重叠，也就是步长为10，这样隐层的神经元个数就是(1000x1000 )/ (10x10)=100x100个神经元了。

那重叠了怎么算？比如上面图的C2中28*28是如何得来的？这里的步长就是1，窗口大小是5*5，所以窗口滑动肯定发生了重叠。下图解释了28的由来。
 

 

第三个问题：S2层是一个下采样层是干嘛用的？为什么是下采样？
也就是上面所说的池化层，只是叫法不同而已。这层利用图像局部相关性的原理，对图像进行子抽样，可以减少数据处理量同时保留有用信息，相当于图像压缩。
  
